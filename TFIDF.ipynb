{"cells":[{"cell_type":"code","source":["# Download modules and install necessary packages\n\n%pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cfff23cc-43ee-474d-8dc4-c89df627ea61","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nRequirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-4485297c-d3f2-466b-8ab1-6588273ca1a4/lib/python3.9/site-packages (3.8.1)\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/envs/pythonEnv-4485297c-d3f2-466b-8ab1-6588273ca1a4/lib/python3.9/site-packages (from nltk) (4.65.0)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.3)\nRequirement already satisfied: regex>=2021.8.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-4485297c-d3f2-466b-8ab1-6588273ca1a4/lib/python3.9/site-packages (from nltk) (2023.3.23)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.0.1)\nPython interpreter will be restarted.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#Import modules"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c98e5aad-a641-48cb-a307-5ba9fbb8f332","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Import modules needed for TF-IDF and data preprocessing\n\nimport nltk\nfrom pyspark.sql import SparkSession\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nspark = SparkSession.builder.appName(\"612_Proj\").config(\"spark.task.cpus\", \"2\").getOrCreate()\nfrom nltk.tokenize import word_tokenize\nfrom pyspark.sql.functions import udf, split, col, concat_ws\nfrom pyspark.sql.types import ArrayType, StringType, IntegerType\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom pyspark.ml.feature import IDF, CountVectorizer,VectorAssembler"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91bc1b7c-279e-41f2-b60c-dbab0566eff9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"]}],"execution_count":0},{"cell_type":"code","source":["# This function loads a CSV file into a dataframe\n#\n# Parameters:\n#    filename: name of the file to be opened\n#\n# Returns:\n#    a pyspark dataframe\n#\ndef csv_to_df(fname):\n    \n    #Location of dbfs\n    dbfs_loc = \"dbfs:/FileStore/shared_uploads/sam.rainbow@ucalgary.ca/\"\n    filename_complete = dbfs_loc + fname\n    filetype=\"csv\"\n    \n    #Options for loading\n    inf_sch = \"true\"\n    is_header = \"true\"\n    delim = \",\"\n    multiline = \"true\"\n    escape = \"\\\"\"\n    \n    # Load the data into a dataframe using options above\n    df = spark.read.format(filetype)\\\n                .option(\"header\", is_header)\\\n                .option(\"inferSchema\", inf_sch)\\\n                .option(\"multiline\", multiline)\\\n                .option(\"escape\", escape).load(filename_complete)\n    \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d81fe74b-39c9-483e-b30f-8b6f1882d685","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Load the CSV in to a dataframe\nall_reviews = csv_to_df(\"IMDB_dataset_rated_final.csv\")\n\n# Concatenate review title and review content with space inbetween\nall_reviews = all_reviews.withColumn(\"Complete_Content\", concat_ws(\" \", \"Review Title\", \"Review Content\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9084164c-6c2c-4d56-863f-0822bde359a3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Cleans the text by removing non-alphabetic characters and whitespace\ndef clean_text(text):\n    text = ''.join(c for c in text if c.isalpha() or c.isspace())\n    text = text.lower()\n    return text\n\n# Function to remove stop words by tokenizing the words and comparing it to words in the nltk corpus\ndef remove_stopwords(text):\n    stop = set(nltk.corpus.stopwords.words('english'))\n    tokens = word_tokenize(text)\n    filtered_tokens = [token for token in tokens if token.lower() not in stop]\n    \n    return filtered_tokens\n\n# Function to lemmatize tokens with POS using WordNetLemmatizer\ndef lemmatize_text_pos(filtered_tokens):\n    lemmatizer = WordNetLemmatizer()\n    pos_tagged = nltk.pos_tag(filtered_tokens)\n    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) if get_wordnet_pos(pos) else token for token, pos in pos_tagged]\n    return lemmatized_tokens\n\n# This function completes stemming on the tokenized text using snowball stemmer\ndef stemmer_snowball(filtered_tokens):\n    snowball = SnowballStemmer(language = 'english')\n    print(filtered_tokens)\n    stemmed_words = [snowball.stem(word) for word in filtered_tokens]\n    return stemmed_words\n\n# Helper function to get the WordNet POS tag from the NLTK POS tag\ndef get_wordnet_pos(nltk_pos_tag):\n    if nltk_pos_tag.startswith('J'):\n        return 'a'\n    elif nltk_pos_tag.startswith('V'):\n        return 'v'\n    elif nltk_pos_tag.startswith('N'):\n        return 'n'\n    elif nltk_pos_tag.startswith('R'):\n        return 'r'\n    else:\n        return None\n\n# Function that applies the cleaning, stop word removal, and lemmatization functions\ndef preprocess_text(text):\n    cleaned = clean_text(text)\n    filtered = remove_stopwords(cleaned)\n    lemmatized = lemmatize_text_pos(filtered)\n    return lemmatized\n\n# Function that applies cleaning, remove stopwords, and stemming\ndef preprocess_text_stemming(text):\n    cleaned = clean_text(text)\n    filtered = remove_stopwords(cleaned)\n    stemmed = stemmer_snowball(filtered)\n    return stemmed\n\n# Default of preprocessing text using lemmatization\npreprocess_text_udf = udf(preprocess_text, ArrayType(StringType()))\n\n# Preprocessing text using snowball stemming\npreprocess_text_udf_stemmed = udf(preprocess_text_stemming, ArrayType(StringType()))\n\n# Apply the user-defined function to the \"Review Content\" column and create a new column \"lemmatized_text\" and \"stemmed_text\"\ndf = all_reviews.withColumn(\"lemmatized_text\", preprocess_text_udf(\"Complete_Content\"))\ndf = df.withColumn(\"stemmed_text\", preprocess_text_udf_stemmed(\"Complete_Content\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d99f839b-e570-4b07-beb1-0d6918d0ed33","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a new column \"Month\" by splitting the \"Review Date\" column on dash and accessing the second element\ndf = df.withColumn(\"Month\", split(col(\"Review Date\"), \"-\")[1])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74a9f7fa-6d7f-4ba4-86b9-4df0dd9ae726","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Converts the lemmatized text to a vector and fits it to and IDF function\n\n# Create a CountVectorizer object for the lemmatized words\ncv = CountVectorizer(inputCol=\"lemmatized_text\", outputCol=\"lemmatized_text_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(df)\n\n# Define an IDF function to compute the inverse document frequency on the lemmatized text\nidf = IDF(inputCol=\"lemmatized_text_vector\", outputCol=\"TFIDF_features_uni-gram\")\n\n# Fit the IDF function to the dataframe to compute the IDF values\nidf_model = idf.fit(count_vectorized_df)\n\n# Apply the IDF model to the dataframe to create a new column with the normalized features\ndf_tfidf = idf_model.transform(count_vectorized_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d6c947b2-7348-4c6c-a4af-e6c701997ccd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Converts the Stemmed text to a vector and fits it to and IDF function\n\n# Create a CountVectorizer object for the lemmatized words\ncv = CountVectorizer(inputCol=\"stemmed_text\", outputCol=\"stemmed_text_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(df_tfidf)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(df_tfidf)\n\n# define an IDF function to compute the inverse document frequency\nidf = IDF(inputCol=\"stemmed_text_vector\", outputCol=\"TFIDF_features_uni-gram_stemmed\")\n\n# fit the IDF function to the dataframe to compute the IDF values\nidf_model = idf.fit(count_vectorized_df)\n\n# apply the IDF model to the dataframe to create a new column with the normalized features\ndf_tfidf = idf_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d31239b5-739a-43b3-b402-1755baa197be","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Retrieves just the month from the dataframe and vectorizes it using CountVectorizer\n\n# Retrieves just the month by splitting on the \",\"\ncount_vectorized_df = df_tfidf.withColumn(\"Month_Vector\", split(count_vectorized_df[\"Month\"], \",\"))\n\n# Create a CountVectorizer object for the month\ncv = CountVectorizer(inputCol=\"Month_Vector\", outputCol=\"Month_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a250fff-6f85-4aaa-aad0-fe884dae215c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Seperate the genre into a new column and vectorizes the genre similar to the month vectorization above\n\n# Seperate genre into new column and preprocesses it\ncount_vectorized_df = count_vectorized_df.withColumn(\"genre_lemmatized\", preprocess_text_udf(\"Movie Genre\"))\n\n# Create a CountVectorizer object for the genre\ncv = CountVectorizer(inputCol=\"genre_lemmatized\", outputCol=\"genre_lemmatized_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"637b46d1-6c0b-4b29-9ca1-d1d721f4b2e3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Assembles all feature vectors created above into one using vector assembler (month, genre, and TFIDF)\n\n# Create a VectorAssembler object for both the stemmed and lemmatized text\nassembler = VectorAssembler(inputCols=[\"TFIDF_features_uni-gram\", \"Month_vector\", \"genre_lemmatized_vector\"], outputCol=\"combined_vectors\")\nassembler1 = VectorAssembler(inputCols=[\"TFIDF_features_uni-gram_stemmed\", \"Month_vector\", \"genre_lemmatized_vector\"], outputCol=\"combined_vectors_stem\")\n\n# Apply the assembler to the DataFrame\nassembled_df = assembler.transform(count_vectorized_df)\nassembled_df = assembler1.transform(assembled_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34acf076-384d-40e5-9e59-f56f035e9ed8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Creates a data frame that only contains the data the TFIDF model will use (combined_vectors) . The \"Complete_Content\" column is included to be able to compare the models predictions back to the manually labelled sentiment to be able to address misclassification.\n\nassembled_df = assembled_df.select(assembled_df[\"combined_vectors\"],assembled_df[\"Manual_Combined\"],assembled_df[\"Complete_Content\"],assembled_df[\"combined_vectors_stem\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a307bca-3284-43dd-a825-3fce0c564450","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Function that increase the raters combined sentiment from a scale of -1,0,1 to 0,1,2 by incrementing each score by 1.\n\nfrom pyspark.sql.functions import col\n\n# This function increases a column value by 1. \n#   Requires the dataframe and the name of the column to be increased.\n#   Returns a new df.\ndef increment_column(df, column_name):\n    incremented_col = col(column_name) + 1\n    new_df = df.withColumn(column_name, incremented_col)\n    \n    return new_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9ff864b-6d12-4837-a932-f9e9f5279e71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Calls the increment_column function to increase the manual rating by 1\nassembled_df = increment_column(assembled_df, \"Manual_Combined\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ec42df8-d597-4113-a22f-958f44a5dd5e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Splits the data into a training and testing set based on a 70/30 split as is a commonly used ratio.\n(training, testing) = assembled_df.randomSplit([0.7, 0.3])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fdb3f7fa-718f-4b82-9c71-1d73456c9942","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# ML Model Training and Evaluation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"361811a4-77a2-44f4-9a1b-54c5ec823ba3","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#1. Naive Bayes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c7b60094-f419-430b-97e3-1ea440cf311f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####     Using Lemmatization"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"21e5bec3-a413-4e83-8c06-76764a9ec43b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Import the necessary models\nfrom pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# Create initial Naïve Bayes model\nnb = NaiveBayes(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors\", modelType=\"multinomial\")\n\n# Evaluate model using a MulticlassificationEvaluator\nnbevaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\n# Set the parameters to be passed to the model in the GridBuilder\nnb_params = [0.2,0.5, 0.7, 0.9]\n\n# Initialize empty lists to house the scores of the model with the given parameters\nnb_f1_scores =[]\nnb_recall_scores = []\nnb_precision_scores = []\n\n# This loops through the parameters and fits the model using the parameters specified above\nfor i in nb_params:\n    \n    #Create ParamGrid for Cross Validation\n    nbparamGrid = (ParamGridBuilder()\n                   .addGrid(nb.smoothing, [i])\n                   .build())\n\n\n    #Create 5-fold CrossValidator\n    nbcv = CrossValidator(estimator = nb,\n                        estimatorParamMaps = nbparamGrid,\n                        evaluator = nbevaluator,\n                        numFolds = 5)\n    \n    # Fit the model and creat a prediction\n    nbcvModel = nbcv.fit(training)\n    prediction = nbcvModel.transform(testing).cache()\n\n    # The three code snippets below populate the evaluation score lists initialized above.\n    evaluator_f1_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n    nb_f1_score = evaluator_f1_nb.evaluate(prediction)\n    nb_f1_scores.append(nb_f1_score)\n\n    evaluator_recall_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n    nb_recall_score = evaluator_recall_nb.evaluate(prediction, {evaluator_recall_nb.metricName: \"weightedRecall\"})\n    nb_recall_scores.append(nb_recall_score)\n\n    evaluator_precision_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n    nb_precision_score = evaluator_precision_nb.evaluate(prediction, {evaluator_precision_nb.metricName: \"weightedPrecision\"})\n    nb_precision_scores.append(nb_precision_score)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"81f74fc9-4982-4471-bd15-41d227197ac8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# This code prints the \n\ntotal_result = []\nfor i in range(4):\n    templist = []\n    templist.append(nb_params[i])\n    templist.append(nb_f1_scores[i])\n    templist.append(nb_recall_scores[i])\n    templist.append(nb_precision_scores[i])\n    total_result.append(templist)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nschema = StructType([\n    StructField(\"Alpha\", StringType(), True),\n    StructField(\"f1_score\", StringType(), True),\n    StructField(\"recall_score\", StringType(), True),\n    StructField(\"precision_score\", StringType(), True)\n])\n\ndf_nb_results = spark.createDataFrame(data=total_result, schema=schema)\ndf_nb_results.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0569bf47-e0a9-4d11-b7e7-5d6fc149c1ff","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0.2","0.610319694984813","0.6101694915254238","0.6121079133760268"],["0.5","0.6194127461694763","0.6169491525423729","0.6272766502685441"],["0.7","0.6208831898351024","0.6169491525423729","0.6326265733045394"],["0.9","0.6140400124153416","0.6101694915254238","0.6262733574760508"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Alpha","type":"\"string\"","metadata":"{}"},{"name":"f1_score","type":"\"string\"","metadata":"{}"},{"name":"recall_score","type":"\"string\"","metadata":"{}"},{"name":"precision_score","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Alpha</th><th>f1_score</th><th>recall_score</th><th>precision_score</th></tr></thead><tbody><tr><td>0.2</td><td>0.610319694984813</td><td>0.6101694915254238</td><td>0.6121079133760268</td></tr><tr><td>0.5</td><td>0.6194127461694763</td><td>0.6169491525423729</td><td>0.6272766502685441</td></tr><tr><td>0.7</td><td>0.6208831898351024</td><td>0.6169491525423729</td><td>0.6326265733045394</td></tr><tr><td>0.9</td><td>0.6140400124153416</td><td>0.6101694915254238</td><td>0.6262733574760508</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Stemming"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"644685e8-574a-4529-82e9-ee5fff0dd39c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n\n# Create initial Naïve Bayes model\nnb = NaiveBayes(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors_stem\", modelType=\"multinomial\")\n\n\n# Evaluate model using a MulticlassificationEvaluator\nnbevaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\n# Set the parameters to be passed to the model in the GridBuilder\nnb_params = [0.2,0.5, 0.7, 0.9]\n\n# Initialize empty lists to house the scores of the model with the given parameters\nnb_f1_scores =[]\nnb_recall_scores = []\nnb_precision_scores = []\n\n# This loops through the parameters and fits the model using the parameters specified above\nfor i in nb_params:\n    \n    #Create ParamGrid for Cross Validation\n    nbparamGrid = (ParamGridBuilder()\n                   .addGrid(nb.smoothing, [i])\n                   .build())\n\n\n    #Create 5-fold CrossValidator\n    nbcv = CrossValidator(estimator = nb,\n                        estimatorParamMaps = nbparamGrid,\n                        evaluator = nbevaluator,\n                        numFolds = 5)\n    \n    # Fit the model and creat a prediction\n    nbcvModel = nbcv.fit(training)\n    prediction = nbcvModel.transform(testing).cache()\n\n    # The three code snippets below populate the evaluation score lists initialized above.\n    evaluator_f1_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n    nb_f1_score = evaluator_f1_nb.evaluate(prediction)\n    nb_f1_scores.append(nb_f1_score)\n\n    evaluator_recall_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n    nb_recall_score = evaluator_recall_nb.evaluate(prediction, {evaluator_recall_nb.metricName: \"weightedRecall\"})\n    nb_recall_scores.append(nb_recall_score)\n\n    evaluator_precision_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n    nb_precision_score = evaluator_precision_nb.evaluate(prediction, {evaluator_precision_nb.metricName: \"weightedPrecision\"})\n    nb_precision_scores.append(nb_precision_score)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0484efe-5f74-4de2-b69f-5bf92df3186e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["total_result = []\nfor i in range(3):\n    templist = []\n    templist.append(nb_params[i])\n    templist.append(nb_f1_scores[i])\n    templist.append(nb_recall_scores[i])\n    templist.append(nb_precision_scores[i])\n    total_result.append(templist)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nschema = StructType([\n    StructField(\"Alpha\", StringType(), True),\n    StructField(\"f1_score\", StringType(), True),\n    StructField(\"recall_score\", StringType(), True),\n    StructField(\"precision_score\", StringType(), True)\n])\n\ndf_nb_results = spark.createDataFrame(data=total_result, schema=schema)\ndf_nb_results.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c26c49d-9a68-4ffc-a5ab-0a679fa8cb5b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0.2","0.5779258611209561","0.5830508474576271","0.578988635428823"],["0.5","0.5818780722540237","0.5830508474576271","0.5862975850977501"],["0.7","0.5777112806533977","0.5796610169491525","0.5807747676429017"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Alpha","type":"\"string\"","metadata":"{}"},{"name":"f1_score","type":"\"string\"","metadata":"{}"},{"name":"recall_score","type":"\"string\"","metadata":"{}"},{"name":"precision_score","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Alpha</th><th>f1_score</th><th>recall_score</th><th>precision_score</th></tr></thead><tbody><tr><td>0.2</td><td>0.5779258611209561</td><td>0.5830508474576271</td><td>0.578988635428823</td></tr><tr><td>0.5</td><td>0.5818780722540237</td><td>0.5830508474576271</td><td>0.5862975850977501</td></tr><tr><td>0.7</td><td>0.5777112806533977</td><td>0.5796610169491525</td><td>0.5807747676429017</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#2. Logistic Regression"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"25fea082-5fdb-4f8f-8db2-e07c9e94c6e2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### Lemmatization"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9d9cea7-b0e3-475b-b2d7-af5cc5dbde71","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial Logistic Regression model\nlr = LogisticRegression(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors\")\n\n# Evaluate model using a MulticlassificationEvaluator\nlrevaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\n# Set the parameters to be passed to the model in the GridBuilder\nparams_lr = [0.005, 0.008,0.001]\n\n# Initialize empty lists to house the scores of the model with the given parameters\nf1_scores_lr =[]\nrecall_scores_lr = []\nprecision_scores_lr = []\n\n# This loops through the parameters and fits the model using the parameters specified above\nfor i in params_lr:\n    # Create ParamGrid for Cross Validation\n    lrparamGrid = (ParamGridBuilder()\n                   .addGrid(lr.regParam, [i])\n                   .build())\n\n\n    # Create 5-fold CrossValidator\n    lrcv = CrossValidator(estimator = lr,\n                        estimatorParamMaps = lrparamGrid,\n                        evaluator = lrevaluator,\n                        numFolds = 5)\n    \n    # Fit the model and create a prediction\n    lrcvModel = lrcv.fit(training)\n    prediction = lrcvModel.transform(testing).cache()\n\n    # The three code snippets below populate the evaluation score lists initialized above.\n    evaluator_f1_lr = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n    lr_f1_score = evaluator_f1_lr.evaluate(prediction)\n    f1_scores_lr.append(lr_f1_score)\n\n    evaluator_recall_lr = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n    lr_recall_score = evaluator_recall_lr.evaluate(prediction, {evaluator_recall_lr.metricName: \"weightedRecall\"})\n    recall_scores_lr.append(lr_recall_score)\n\n    evaluator_precision_lr = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n    lr_precision_score = evaluator_precision_lr.evaluate(prediction, {evaluator_precision_lr.metricName: \"weightedPrecision\"})\n    precision_scores_lr.append(lr_precision_score)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8cc07db-cc66-4895-ac27-e7a8f381c36d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["lr_total_result = []\nfor i in range(3):\n    templist = []\n    templist.append(params_lr[i])\n    templist.append(f1_scores_lr[i])\n    templist.append(recall_scores_lr[i])\n    templist.append(precision_scores_lr[i])\n    lr_total_result.append(templist)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nschema = StructType([\n    StructField(\"Alpha\", StringType(), True),\n    StructField(\"f1_score\", StringType(), True),\n    StructField(\"recall_score\", StringType(), True),\n    StructField(\"precision_score\", StringType(), True)\n])\n\ndf_nb_results = spark.createDataFrame(data=lr_total_result, schema=schema)\ndf_nb_results.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fdbb690-eefa-4744-a2b8-a44233ff3164","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0.005","0.6426088447607181","0.688135593220339","0.6434385227363435"],["0.008","0.6447928280045218","0.6915254237288135","0.6513013916403747"],["0.001","0.6385692439575567","0.6813559322033899","0.6300033077644537"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Alpha","type":"\"string\"","metadata":"{}"},{"name":"f1_score","type":"\"string\"","metadata":"{}"},{"name":"recall_score","type":"\"string\"","metadata":"{}"},{"name":"precision_score","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Alpha</th><th>f1_score</th><th>recall_score</th><th>precision_score</th></tr></thead><tbody><tr><td>0.005</td><td>0.6426088447607181</td><td>0.688135593220339</td><td>0.6434385227363435</td></tr><tr><td>0.008</td><td>0.6447928280045218</td><td>0.6915254237288135</td><td>0.6513013916403747</td></tr><tr><td>0.001</td><td>0.6385692439575567</td><td>0.6813559322033899</td><td>0.6300033077644537</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Stemming"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3b54f95d-9fa1-4adf-98b9-dfdec11a3797","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Create initial Logistic Regression model\nlr = LogisticRegression(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors_stem\")\n\n# Evaluate model using a MulticlassificationEvaluator\nlrevaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\n# Set the parameters to be passed to the model in the GridBuilder\nparams_lr = [0.005, 0.008,0.001]\n\n# Initialize empty lists to house the scores of the model with the given parameters\nf1_scores_lr =[]\nrecall_scores_lr = []\nprecision_scores_lr = []\n\n# This loops through the parameters and fits the model using the parameters specified above\nfor i in params_lr:\n    # Create ParamGrid for Cross Validation\n    lrparamGrid = (ParamGridBuilder()\n                   .addGrid(lr.regParam, [i])\n                   .build())\n\n\n    # Create 5-fold CrossValidator\n    lrcv = CrossValidator(estimator = lr,\n                        estimatorParamMaps = lrparamGrid,\n                        evaluator = lrevaluator,\n                        numFolds = 5)\n    \n    # Fit the model and create a prediction\n    lrcvModel = lrcv.fit(training)\n    prediction = lrcvModel.transform(testing).cache()\n\n    # The three code snippets below populate the evaluation score lists initialized above.\n    evaluator_f1_lr = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n    lr_f1_score = evaluator_f1_lr.evaluate(prediction)\n    f1_scores_lr.append(lr_f1_score)\n\n    evaluator_recall_lr = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n    lr_recall_score = evaluator_recall_lr.evaluate(prediction, {evaluator_recall_lr.metricName: \"weightedRecall\"})\n    recall_scores_lr.append(lr_recall_score)\n\n    evaluator_precision_lr = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n    lr_precision_score = evaluator_precision_lr.evaluate(prediction, {evaluator_precision_lr.metricName: \"weightedPrecision\"})\n    precision_scores_lr.append(lr_precision_score)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"673d8e56-e8bb-4a46-a27d-2642d5dcc38c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["lr_total_result = []\nfor i in range(3):\n    templist = []\n    templist.append(params_lr[i])\n    templist.append(f1_scores_lr[i])\n    templist.append(recall_scores_lr[i])\n    templist.append(precision_scores_lr[i])\n    lr_total_result.append(templist)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nschema = StructType([\n    StructField(\"Alpha\", StringType(), True),\n    StructField(\"f1_score\", StringType(), True),\n    StructField(\"recall_score\", StringType(), True),\n    StructField(\"precision_score\", StringType(), True)\n])\n\ndf_nb_results = spark.createDataFrame(data=lr_total_result, schema=schema)\ndf_nb_results.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df3051d2-2170-4b19-9767-82fdf4dd0055","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["0.005","0.6669253844994709","0.6983050847457627","0.6797047555939275"],["0.008","0.6694688629396821","0.7016949152542373","0.6866921428464385"],["0.001","0.650238599987067","0.6813559322033897","0.663986328894413"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Alpha","type":"\"string\"","metadata":"{}"},{"name":"f1_score","type":"\"string\"","metadata":"{}"},{"name":"recall_score","type":"\"string\"","metadata":"{}"},{"name":"precision_score","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Alpha</th><th>f1_score</th><th>recall_score</th><th>precision_score</th></tr></thead><tbody><tr><td>0.005</td><td>0.6669253844994709</td><td>0.6983050847457627</td><td>0.6797047555939275</td></tr><tr><td>0.008</td><td>0.6694688629396821</td><td>0.7016949152542373</td><td>0.6866921428464385</td></tr><tr><td>0.001</td><td>0.650238599987067</td><td>0.6813559322033897</td><td>0.663986328894413</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#2. RANDOM FOREST\n\n### Lemmatization|"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"444f660f-cfd6-487c-8f8f-bd13ab109e24","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors\")\n\n# Evaluate model using a MulticlassificationEvaluator\nrf_evaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\n# Set the parameters to be passed to the model in the GridBuilder\nparams_rf_trees = [10, 30, 50]\nparams_rf_maxdepth = [10, 15, 20]\n\n# Initialize empty lists to house the scores of the model with the given parameters\ni=0\nf1_scores_rf =[]\nrecall_scores_rf = []\nprecision_scores_rf = []\n\n# This loops through the parameters and fits the model using the parameters specified above\nfor depth in params_rf_maxdepth:\n    j=0\n    #iterating through number of trees\n    for trees in params_rf_trees:\n        # Create ParamGrid for Cross Validation\n        rfparamGrid = (ParamGridBuilder()\n                       .addGrid(rf.numTrees, [trees]) \\\n        .addGrid(rf.maxDepth, [depth]) \\\n                       .build())\n\n\n        # Create 5-fold CrossValidator\n        rfcv = CrossValidator(estimator = rf,\n                            estimatorParamMaps = rfparamGrid,\n                            evaluator = rf_evaluator,\n                            numFolds = 5)\n        rfcvModel = rfcv.fit(training)\n\n        prediction = rfcvModel.transform(testing).cache()\n\n        #Evaluating F1 score\n        evaluator_f1_rf = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n        f1_score = evaluator_f1_rf.evaluate(prediction)\n        f1_scores_rf.append(f1_score)\n        \n        #evaluating recall score\n        evaluator_recall_rf = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n        recall_score = evaluator_recall_rf.evaluate(prediction, {evaluator_recall_lr.metricName: \"weightedRecall\"})\n        recall_scores_rf.append(recall_score)\n\n        #evaluating precision score\n        evaluator_precision_rf = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n        precision_score = evaluator_precision_rf.evaluate(prediction, {evaluator_precision_lr.metricName: \"weightedPrecision\"})\n        precision_scores_rf.append(precision_score)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c3da533-1932-43b5-af90-1b456be5b371","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["x = 0\nfor i in range(3):\n    for j in range(3):\n        print(\"Max depth is : \" + str(params_rf_maxdepth[i]))\n        print(\"Number of trees is: \" + str(params_rf_trees[j]))\n        print(\"f1 is: \" + str(f1_scores_rf[x]))\n        print(\"recall is: \" + str(recall_scores_rf[x]))\n        print(\"precision is: \" + str(precision_scores_rf[x]))\n        x+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a2a512ba-73cd-4371-8936-ec7e8c5e988b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Max depth is : 10\nNumber of trees is: 10\nf1 is: 0.40367956966482\nrecall is: 0.5050847457627119\nprecision is: 0.4345650959210281\nMax depth is : 10\nNumber of trees is: 30\nf1 is: 0.39030341075538816\nrecall is: 0.5050847457627119\nprecision is: 0.4744684424985912\nMax depth is : 10\nNumber of trees is: 50\nf1 is: 0.4061137059637736\nrecall is: 0.5152542372881356\nprecision is: 0.48513786635518097\nMax depth is : 15\nNumber of trees is: 10\nf1 is: 0.4115146623621199\nrecall is: 0.5016949152542373\nprecision is: 0.4049588377723971\nMax depth is : 15\nNumber of trees is: 30\nf1 is: 0.4608579635822515\nrecall is: 0.5525423728813559\nprecision is: 0.4766076834974385\nMax depth is : 15\nNumber of trees is: 50\nf1 is: 0.4724160035545253\nrecall is: 0.5627118644067797\nprecision is: 0.4927145575670404\nMax depth is : 20\nNumber of trees is: 10\nf1 is: 0.42483413700242567\nrecall is: 0.5050847457627119\nprecision is: 0.40579440879500384\nMax depth is : 20\nNumber of trees is: 30\nf1 is: 0.48555028248587573\nrecall is: 0.5728813559322034\nprecision is: 0.4884889538463978\nMax depth is : 20\nNumber of trees is: 50\nf1 is: 0.5206927102954436\nrecall is: 0.6\nprecision is: 0.717855563743552\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Stemming"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{},"nuid":"b7a12f85-eefe-4ed6-bef9-e755ddb968e4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# RandomForestClassifier\nrf = RandomForestClassifier(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors_stem\")\n\n# Evaluate model using a MulticlassificationEvaluator\nrf_evaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\n# Set the parameters to be passed to the model in the GridBuilder\nparams_rf_trees = [10, 30, 50]\nparams_rf_maxdepth = [10, 15, 20]\n\n# Initialize empty lists to house the scores of the model with the given parameters\ni=0\nf1_scores_rf =[]\nrecall_scores_rf = []\nprecision_scores_rf = []\n\n# This loops through the parameters and fits the model using the parameters specified above\nfor depth in params_rf_maxdepth:\n    j=0\n    #iterating through number of trees\n    for trees in params_rf_trees:\n        # Create ParamGrid for Cross Validation\n        rfparamGrid = (ParamGridBuilder()\n                       .addGrid(rf.numTrees, [trees]) \\\n        .addGrid(rf.maxDepth, [depth]) \\\n                       .build())\n\n\n        # Create 5-fold CrossValidator\n        rfcv = CrossValidator(estimator = rf,\n                            estimatorParamMaps = rfparamGrid,\n                            evaluator = rf_evaluator,\n                            numFolds = 5)\n        rfcvModel = rfcv.fit(training)\n\n        prediction = rfcvModel.transform(testing).cache()\n\n        #Evaluating F1 score\n        evaluator_f1_rf = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n        f1_score = evaluator_f1_rf.evaluate(prediction)\n        f1_scores_rf.append(f1_score)\n        \n        #evaluating recall score\n        evaluator_recall_rf = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n        recall_score = evaluator_recall_rf.evaluate(prediction, {evaluator_recall_lr.metricName: \"weightedRecall\"})\n        recall_scores_rf.append(recall_score)\n\n        #evaluating precision score\n        evaluator_precision_rf = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n        precision_score = evaluator_precision_rf.evaluate(prediction, {evaluator_precision_lr.metricName: \"weightedPrecision\"})\n        precision_scores_rf.append(precision_score)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"397258f3-67c8-4b1b-ba24-fb8c83868495","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["x=0\nfor i in range(3):\n    for j in range(3):\n        print(\"Max depth is : \" + str(params_rf_maxdepth[i]))\n        print(\"Number of trees is: \" + str(params_rf_trees[j]))\n        print(\"f1 is: \" + str(f1_scores_rf[x]))\n        print(\"recall is: \" + str(recall_scores_rf[x]))\n        print(\"precision is: \" + str(precision_scores_rf[x]))\n\n        x+=1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"53996c2a-2bcd-4770-a34c-ea251fa9bb69","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Max depth is : 10\nNumber of trees is: 10\nf1 is: 0.471549034488574\nrecall is: 0.5525423728813559\nprecision is: 0.5815962580577826\nMax depth is : 10\nNumber of trees is: 30\nf1 is: 0.4452514316835278\nrecall is: 0.5457627118644067\nprecision is: 0.5078586355206876\nMax depth is : 10\nNumber of trees is: 50\nf1 is: 0.4114602679039692\nrecall is: 0.5186440677966102\nprecision is: 0.4977845417137207\nMax depth is : 15\nNumber of trees is: 10\nf1 is: 0.4660184347404428\nrecall is: 0.5423728813559322\nprecision is: 0.5457977821973938\nMax depth is : 15\nNumber of trees is: 30\nf1 is: 0.5186582536609874\nrecall is: 0.6033898305084746\nprecision is: 0.5156549701978017\nMax depth is : 15\nNumber of trees is: 50\nf1 is: 0.4885728345662679\nrecall is: 0.576271186440678\nprecision is: 0.5159695524914691\nMax depth is : 20\nNumber of trees is: 10\nf1 is: 0.4875199787985015\nrecall is: 0.5593220338983051\nprecision is: 0.5557642278887844\nMax depth is : 20\nNumber of trees is: 30\nf1 is: 0.5339726499596422\nrecall is: 0.6169491525423728\nprecision is: 0.509909069763903\nMax depth is : 20\nNumber of trees is: 50\nf1 is: 0.5052987158837351\nrecall is: 0.5898305084745763\nprecision is: 0.5005119517049028\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fba11cb3-0535-4d56-933f-0fd5f259aa4d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"TFIDF_Sam_v2 (1)","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1301799772487842}},"nbformat":4,"nbformat_minor":0}
