{"cells":[{"cell_type":"code","source":["# Download modules\n%pip install nltk"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2aa94f07-98e3-422b-b310-87e358b009cc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nCollecting nltk\n  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\nRequirement already satisfied: click in /databricks/python3/lib/python3.9/site-packages (from nltk) (8.0.3)\nCollecting regex>=2021.8.3\n  Using cached regex-2023.3.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (768 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.9/site-packages (from nltk) (1.0.1)\nInstalling collected packages: tqdm, regex, nltk\nSuccessfully installed nltk-3.8.1 regex-2023.3.23 tqdm-4.65.0\nPython interpreter will be restarted.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#Import modules"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"215c297d-a4d8-4ab6-84ae-424761c19f7b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import nltk\nfrom pyspark.sql import SparkSession\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nspark = SparkSession.builder.appName(\"612_Proj\").config(\"spark.task.cpus\", \"2\").getOrCreate()\nfrom nltk.tokenize import word_tokenize\nfrom pyspark.sql.functions import udf, split, col\nfrom pyspark.sql.types import ArrayType, StringType\nfrom pyspark.ml.feature import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom pyspark.sql.functions import concat\nfrom pyspark.sql.functions import concat_ws\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f7ae7a6-45bb-4a88-a5cd-af91d407895d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /root/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /root/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"]}],"execution_count":0},{"cell_type":"code","source":["#change the csv to a dataframe for databricks to use\ndef csv_to_df(fname):\n    \n    #Location of dbfs\n    dbfs_loc = \"dbfs:/FileStore/tables/\"\n    filename_complete = dbfs_loc + fname\n    filetype=\"csv\"\n    \n    #Options for loading\n    inf_sch = \"true\"\n    is_header = \"true\"\n    delim = \",\"\n    multiline = \"true\"\n    escape = \"\\\"\"\n    \n    # Load the data into a dataframe using options above\n    df = spark.read.format(filetype)\\\n                .option(\"header\", is_header)\\\n                .option(\"inferSchema\", inf_sch)\\\n                .option(\"multiline\", multiline)\\\n                .option(\"escape\", escape).load(filename_complete)\n    \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"738713e2-20cc-40b4-95f5-8677c996993c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Load CSVs\nall_reviews = csv_to_df(\"IMDB_dataset_rated_final.csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cbc1bb80-6b1d-4ed1-81db-664a08bddb1a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#concatenate review title and review content with space inbetween\nall_reviews = all_reviews.withColumn(\"Complete_Content\", concat_ws(\" \", \"Review Title\", \"Review Content\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3651dea5-03ae-4dab-94f3-d95d188a8736","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from nltk.stem import SnowballStemmer\n# Cleans the text by removing non-alphabet characters or whitespace\ndef clean_text(text):\n    text = ''.join(c for c in text if c.isalpha() or c.isspace())\n    text = text.lower()\n    return text\n\n# Function to remove stop words\ndef remove_stopwords(text):\n    stop = set(nltk.corpus.stopwords.words('english'))\n    tokens = word_tokenize(text)\n    filtered_tokens = [token for token in tokens if token.lower() not in stop]\n    \n    return filtered_tokens\n\n# Function to lemmatize tokens with POS\ndef lemmatize_text_pos(filtered_tokens):\n    lemmatizer = WordNetLemmatizer()\n    pos_tagged = nltk.pos_tag(filtered_tokens)\n    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) if get_wordnet_pos(pos) else token for token, pos in pos_tagged]\n    return lemmatized_tokens\n  \ndef stemmer_snowball(filtered_tokens):\n    snowball = SnowballStemmer(language = 'english')\n    print(filtered_tokens)\n    stemmed_words = [snowball.stem(word) for word in filtered_tokens]\n    return stemmed_words\n\n# Helper function to get the WordNet POS tag from the NLTK POS tag\ndef get_wordnet_pos(nltk_pos_tag):\n    if nltk_pos_tag.startswith('J'):\n        return 'a'\n    elif nltk_pos_tag.startswith('V'):\n        return 'v'\n    elif nltk_pos_tag.startswith('N'):\n        return 'n'\n    elif nltk_pos_tag.startswith('R'):\n        return 'r'\n    else:\n        return None\n\n# Function that applies the cleaning, stop word removal, and lemmatization functions\ndef preprocess_text(text):\n#     if text is None or not isinstance(text, str):\n#         return []\n    cleaned = clean_text(text)\n    filtered = remove_stopwords(cleaned)\n    lemmatized = lemmatize_text_pos(filtered)\n    print(\"this is sam\")\n    return lemmatized\n\n#Function applies cleaning, stop word removal and snowballstemming\ndef preprocess_text_stemming(text):\n#     if text is None or not isinstance(text, str):\n#         return []\n    cleaned = clean_text(text)\n    filtered = remove_stopwords(cleaned)\n    stemmed = stemmer_snowball(filtered)\n    return stemmed\n\n#default of preprocessing text using lemmatization\npreprocess_text_udf = udf(preprocess_text, ArrayType(StringType()))\n\n#preprocessing text using snowball stemming\npreprocess_text_udf_stemmed = udf(preprocess_text_stemming, ArrayType(StringType()))\n\n# Apply the user-defined function to the \"Review Content\" column and create a new column \"lemmatized_text\"\ndf = all_reviews.withColumn(\"lemmatized_text\", preprocess_text_udf(\"Complete_Content\"))\ndf = df.withColumn(\"stemmed_text\", preprocess_text_udf_stemmed(\"Complete_Content\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e9ee476-b99b-4622-b1be-87b59af08374","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Converts the lemmatized text to a vector and fits it to and IDF function\nfrom pyspark.ml.feature import IDF, CountVectorizer,VectorAssembler\n# Create a CountVectorizer object for the lemmatized words\ncv = CountVectorizer(inputCol=\"lemmatized_text\", outputCol=\"lemmatized_text_vector_idf\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(df)\n\n# Define an IDF function to compute the inverse document frequency on the lemmatized text\nidf = IDF(inputCol=\"lemmatized_text_vector_idf\", outputCol=\"TFIDF_features_uni-gram_lemm\")\n\n# Fit the IDF function to the dataframe to compute the IDF values\nidf_model = idf.fit(count_vectorized_df)\n\n# Apply the IDF model to the dataframe to create a new column with the normalized features\ndf = idf_model.transform(count_vectorized_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85c6c15a-735c-4f51-97cc-8f8cc98d463e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Converts the Stemmed text to a vector and fits it to and IDF function\n\n# Create a CountVectorizer object for the lemmatized words\ncv = CountVectorizer(inputCol=\"stemmed_text\", outputCol=\"stemmed_text_vector_idf\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(df)\n\n# define an IDF function to compute the inverse document frequency\nidf = IDF(inputCol=\"stemmed_text_vector_idf\", outputCol=\"TFIDF_features_uni-gram_stemmed\")\n\n# fit the IDF function to the dataframe to compute the IDF values\nidf_model = idf.fit(count_vectorized_df)\n\n# apply the IDF model to the dataframe to create a new column with the normalized features\ndf = idf_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc5230e1-762b-4652-92e2-c2a9114779df","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# create a new column \"Month\" by splitting the \"Review Date\" column on dash and accessing the second element\ndf = df.withColumn(\"Month\", split(col(\"Review Date\"), \"-\")[1])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b122761-da74-4b33-9f2d-0d2afd960139","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#seperate genre into new column\ndf = df.withColumn(\"genre_lemmatized\", preprocess_text_udf(\"Movie Genre\"))\ndf = df.withColumn(\"genre_stemmed\", preprocess_text_udf_stemmed(\"Movie Genre\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a54f5972-197a-4bac-81bb-59b10136a3fa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\n\n#bi-gram functions\ndf = df.withColumn('new_lemmatized_text', concat_ws(' ', 'lemmatized_text'))\ntokenizer = Tokenizer(inputCol=\"new_lemmatized_text\", outputCol=\"words_2\")\n\nn = 2\ndf_words = tokenizer.transform(df)\n\ndef get_bigrams(words):\n    ngrams = []\n    for i in range(len(words) - n + 1):\n        ngrams.append(' '.join(words[i:i+n]))\n    return ngrams\n\n\nget_ngrams_udf = udf(get_bigrams, ArrayType(StringType()))\n\n\ndf = df_words.withColumn(\"bi_grams_lemm\", get_ngrams_udf(\"words_2\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e7137b12-d5f5-45c0-8611-e955066193c9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import ArrayType, StringType\n\n#same as above\ndf = df.withColumn('new_stemmed_text', concat_ws(' ', 'stemmed_text'))\ntokenizer = Tokenizer(inputCol=\"new_stemmed_text\", outputCol=\"words_2s\")\n\nn = 2\ndf_words = tokenizer.transform(df)\n\ndef get_bigrams(words):\n    ngrams = []\n    for i in range(len(words) - n + 1):\n        ngrams.append(' '.join(words[i:i+n]))\n    return ngrams\n\n\nget_ngrams_udf = udf(get_bigrams, ArrayType(StringType()))\n\n\ndf = df_words.withColumn(\"bi_grams_stem\", get_ngrams_udf(\"words_2s\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aef0c635-add1-4f21-ab42-7dd8eed4a41e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Generating vector of bi-grams"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d0b8b30-7072-45b4-ba39-ffdc1ffbb6dd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#CV fOR bigram for lemmatized text\n# Create a CountVectorizer object for the lemmatized words\ncv = CountVectorizer(inputCol=\"bi_grams_lemm\", outputCol=\"bi_grams_lemmatized_text_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"90222322-a1ce-4537-9d54-2720da18ed38","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#CV fOR bigram for stemmed text\n# Create a CountVectorizer object for the stemmed words\ncv = CountVectorizer(inputCol=\"bi_grams_stem\", outputCol=\"bi_grams_stemmed_text_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a576198-4bf8-4c8f-b75d-2b9051727322","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Generating vector of reviews (bag of words)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3fedef23-9334-4025-9b37-f033ba330056","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#CV fOR BoW for lemmatized text\n# Create a CountVectorizer object for the lemmatized words\ncv = CountVectorizer(inputCol=\"lemmatized_text\", outputCol=\"lemmatized_text_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63796edd-b9ae-4b38-83d7-0a746ac5bde8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#CV fOR BoW for stemmed text\nfrom pyspark.ml.feature import IDF\n\n# Create a CountVectorizer object for the stemmed words\ncv = CountVectorizer(inputCol=\"stemmed_text\", outputCol=\"stemmed_text_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"55ea9c16-b3d9-4e57-97b1-1af7a64c11ab","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Generating Vectors of Genres"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ee324afe-11ad-4fdd-ba25-eeb7c0451417","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Create a CountVectorizer object for the genre\ncv = CountVectorizer(inputCol=\"genre_lemmatized\", outputCol=\"genre_lemmatized_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df47b2e2-c807-437f-a979-77fa6afb9939","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a CountVectorizer object for the genre using stemming\ncv = CountVectorizer(inputCol=\"genre_stemmed\", outputCol=\"genre_stemmed_vector\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a489134c-62be-4898-b908-21e074d1ae52","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["count_vectorized_df = count_vectorized_df.withColumn(\"Month_Vector\", split(count_vectorized_df[\"Month\"], \",\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a7e5c08-b40c-4f94-b8a4-c012366fed00","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Generating vectors for month"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff64c428-65c9-4002-96f0-4f96a916f2c8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Create a CountVectorizer object for the month\ncv = CountVectorizer(inputCol=\"Month_Vector\", outputCol=\"Month_vectorized\")\n\n# Fit the CountVectorizer model on the dataframe\ncv_model = cv.fit(count_vectorized_df)\n\n# Transform the dataframe to add the count vectorized features column\ncount_vectorized_df = cv_model.transform(count_vectorized_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b28bbb9c-5bea-4dc2-bc97-f32f83d2ad8c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["####Combining vectors of bag of words, bi-gram, TF-IDF, month and genre"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"14fc31cf-719c-49e0-a4a2-8f7401784268","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["#combine all vectors into one vector for ML\nfrom pyspark.ml.feature import VectorAssembler\n\n# Assume you have a Spark DataFrame `df` with two vector columns `vector1` and `vector2`.\n\n# Create a VectorAssembler object\nassembler = VectorAssembler(inputCols=[\"bi_grams_lemmatized_text_vector\", \"Month_vectorized\", \"genre_lemmatized_vector\", \"lemmatized_text_vector\", \"TFIDF_features_uni-gram_lemm\" ], outputCol=\"combined_vectors_lem\")\nassembler1 = VectorAssembler(inputCols=[\"bi_grams_stemmed_text_vector\", \"Month_vectorized\", \"genre_lemmatized_vector\", \"stemmed_text_vector\", \"TFIDF_features_uni-gram_stemmed\"], outputCol=\"combined_vectors_stem\")\n\n# Apply the assembler to the DataFrame\nassembled_df = assembler.transform(count_vectorized_df)\nassembled_df = assembler1.transform(assembled_df)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"925aeec1-b2c5-4337-855e-931c750f3696","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_test = assembled_df.select(assembled_df[\"combined_vectors_lem\"],assembled_df[\"combined_vectors_stem\"],assembled_df[\"Manual_Combined\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e2e27c7-40a5-422a-8f55-3cd2bcf5ae60","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\ndef increment_column(df, column_name):\n    # select the column and add 1 to each value\n    incremented_col = col(column_name) + 1    \n    # create a new dataframe with the incremented column\n    new_df = df.withColumn(column_name, incremented_col)\n    \n    return new_df"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"77ead211-648e-4cd2-818a-0499a4653ba8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_test2 = increment_column(df_test, \"Manual_Combined\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c54311cb-b987-4fe3-ad59-b5017aecb0dd","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["(training, testing) = df_test2.randomSplit([0.7, 0.3])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b280d476-f805-4dd3-a7f7-b6dd68fcbc06","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Models"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a34633c5-f248-44f7-b8d6-63e15e8d2248","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["####Naive Bayes"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"02a6d6fb-a591-4d59-9cf5-6681aab125a4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#####using lemmatized text"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ab7d5600-dede-4863-b1a8-3b21f5b7cf07","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n#updated NB\n# Create initial Naïve Bayes model\nnb = NaiveBayes(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors_lem\", modelType=\"multinomial\")\n\n# Evaluate model\nnbevaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\nparams_nb = [0.0001,  0.01, 1.0, 5.0]\n\nf1_scores_nb =[]\nrecall_scores_nb = []\nprecision_scores_nb = []\n\n\nfor i in params_nb:\n        # Create ParamGrid for Cross Validation\n    nbparamGrid = (ParamGridBuilder()\n                   .addGrid(nb.smoothing, [i])\n                   .build())\n\n\n    # Create 5-fold CrossValidator\n    nbcv = CrossValidator(estimator = nb,\n                        estimatorParamMaps = nbparamGrid,\n                        evaluator = nbevaluator,\n                        numFolds = 5)\n    nbcvModel = nbcv.fit(training)\n\n    prediction = nbcvModel.transform(testing).cache()\n\n\n    evaluator_f1_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n    f1_score = evaluator_f1_nb.evaluate(prediction)\n    f1_scores_nb.append(f1_score)\n\n    evaluator_recall_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n    recall_score = evaluator_recall_nb.evaluate(prediction, {evaluator_recall_nb.metricName: \"weightedRecall\"})\n    recall_scores_nb.append(recall_score)\n\n    evaluator_precision_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n    precision_score = evaluator_precision_nb.evaluate(prediction, {evaluator_precision_nb.metricName: \"weightedPrecision\"})\n    precision_scores_nb.append(precision_score)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc539478-8068-4eae-90f2-a538bcce9ee3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Printing NB to dataframe\nnb_total_result = []\nfor i in range(len(params_nb)):\n    templist = []\n    templist.append(params_nb[i])\n    templist.append(f1_scores_nb[i])\n    templist.append(recall_scores_nb[i])\n    templist.append(precision_scores_nb[i])\n    nb_total_result.append(templist)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nschema = StructType([\n    StructField(\"Smoothing\", StringType(), True),\n    StructField(\"f1_score\", StringType(), True),\n    StructField(\"recall_score\", StringType(), True),\n    StructField(\"precision_score\", StringType(), True)\n])\n\ndf_nb_results = spark.createDataFrame(data=nb_total_result, schema=schema)\ndf_nb_results.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f947e9c-41d0-42f8-8572-67dd9cfb325f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["1.0E-4","0.5827081755902929","0.6181102362204725","0.5837107318106975"],["0.01","0.6295205843901643","0.6377952755905512","0.6241296419098143"],["1.0","0.6231347386466997","0.6653543307086615","0.6233490809931812"],["5.0","0.44034121533408876","0.5393700787401574","0.5531496062992126"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Smoothing","type":"\"string\"","metadata":"{}"},{"name":"f1_score","type":"\"string\"","metadata":"{}"},{"name":"recall_score","type":"\"string\"","metadata":"{}"},{"name":"precision_score","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Smoothing</th><th>f1_score</th><th>recall_score</th><th>precision_score</th></tr></thead><tbody><tr><td>1.0E-4</td><td>0.5827081755902929</td><td>0.6181102362204725</td><td>0.5837107318106975</td></tr><tr><td>0.01</td><td>0.6295205843901643</td><td>0.6377952755905512</td><td>0.6241296419098143</td></tr><tr><td>1.0</td><td>0.6231347386466997</td><td>0.6653543307086615</td><td>0.6233490809931812</td></tr><tr><td>5.0</td><td>0.44034121533408876</td><td>0.5393700787401574</td><td>0.5531496062992126</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#####using snowball stemmer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70ad0da9-7914-40c2-9bca-6a8e30462e00","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.classification import NaiveBayes\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n#updated NB\n# Create initial Naïve Bayes model\nnb = NaiveBayes(labelCol=\"Manual_Combined\", featuresCol=\"combined_vectors_stem\", modelType=\"multinomial\")\n\n# Evaluate model\nnbevaluator = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", metricName=\"weightedFMeasure\")\n\nparams_nb = [0.0001,  0.01, 1.0, 5.0]\n\nf1_scores_nb =[]\nrecall_scores_nb = []\nprecision_scores_nb = []\n\n\nfor i in params_nb:\n        # Create ParamGrid for Cross Validation\n    nbparamGrid = (ParamGridBuilder()\n                   .addGrid(nb.smoothing, [i])\n                   .build())\n\n\n    # Create 5-fold CrossValidator\n    nbcv = CrossValidator(estimator = nb,\n                        estimatorParamMaps = nbparamGrid,\n                        evaluator = nbevaluator,\n                        numFolds = 5)\n    nbcvModel = nbcv.fit(training)\n\n    prediction = nbcvModel.transform(testing).cache()\n\n\n    evaluator_f1_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedFMeasure\")\n    f1_score = evaluator_f1_nb.evaluate(prediction)\n    f1_scores_nb.append(f1_score)\n\n    evaluator_recall_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n    recall_score = evaluator_recall_nb.evaluate(prediction, {evaluator_recall_nb.metricName: \"weightedRecall\"})\n    recall_scores_nb.append(recall_score)\n\n    evaluator_precision_nb = MulticlassClassificationEvaluator(labelCol=\"Manual_Combined\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n    precision_score = evaluator_precision_nb.evaluate(prediction, {evaluator_precision_nb.metricName: \"weightedPrecision\"})\n    precision_scores_nb.append(precision_score)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69aaf2cf-d0dd-4e30-915d-ed51d40a9ab1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Printing NB to dataframe\nnb_total_result = []\nfor i in range(len(params_nb)):\n    templist = []\n    templist.append(params_nb[i])\n    templist.append(f1_scores_nb[i])\n    templist.append(recall_scores_nb[i])\n    templist.append(precision_scores_nb[i])\n    nb_total_result.append(templist)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType\n\nschema = StructType([\n    StructField(\"Smoothing\", StringType(), True),\n    StructField(\"f1_score\", StringType(), True),\n    StructField(\"recall_score\", StringType(), True),\n    StructField(\"precision_score\", StringType(), True)\n])\n\ndf_nb_results = spark.createDataFrame(data=nb_total_result, schema=schema)\ndf_nb_results.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cff472a9-7918-445e-8bec-9d4a0ed02de7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["1.0E-4","0.6132175172386799","0.6377952755905512","0.6206866282182955"],["0.01","0.6211514865044401","0.6220472440944882","0.620434442224475"],["1.0","0.6054139156727427","0.6496062992125984","0.6075974016247335"],["5.0","0.4465575435562189","0.5433070866141732","0.5679133858267716"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Smoothing","type":"\"string\"","metadata":"{}"},{"name":"f1_score","type":"\"string\"","metadata":"{}"},{"name":"recall_score","type":"\"string\"","metadata":"{}"},{"name":"precision_score","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Smoothing</th><th>f1_score</th><th>recall_score</th><th>precision_score</th></tr></thead><tbody><tr><td>1.0E-4</td><td>0.6132175172386799</td><td>0.6377952755905512</td><td>0.6206866282182955</td></tr><tr><td>0.01</td><td>0.6211514865044401</td><td>0.6220472440944882</td><td>0.620434442224475</td></tr><tr><td>1.0</td><td>0.6054139156727427</td><td>0.6496062992125984</td><td>0.6075974016247335</td></tr><tr><td>5.0</td><td>0.4465575435562189</td><td>0.5433070866141732</td><td>0.5679133858267716</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ENSF612-combined","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3082022085872169}},"nbformat":4,"nbformat_minor":0}
